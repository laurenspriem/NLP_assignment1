{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f22f426a-344f-4505-b76e-0569027db8cf",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34da11a5-7346-413c-9f64-be7b1e3ef676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import FuzzyTM\n",
    "import contractions\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef432a5",
   "metadata": {},
   "source": [
    "# Stock price dataset\n",
    "- What documents do we include in the corpus?\n",
    "- How do we create the labels for the target variable?\n",
    "- Has the corpus from Kaggle been pre-processed in some way? Do we use other proprocessing steps as well? (use at least stop-word removal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6cb681-224f-4d40-9c88-28be68c561be",
   "metadata": {},
   "source": [
    "## Load stock price data (read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0b16db-49d4-42e9-9a4e-34939773d08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the stock price data from csv\n",
    "df_stock = pd.read_csv(\"../data/raw/AAPL.csv\")\n",
    "df_stock.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef98617-7779-48e1-a289-02717d53feff",
   "metadata": {},
   "source": [
    "## Labeling (preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79db277-ee99-475a-b949-45307034c0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labeling; whenever the closing price is higher than the opening price, assign label 1.\n",
    "df_stock['Label'] = 0\n",
    "df_stock.loc[(df_stock['Close'] - df_stock['Open']) > 0, 'Label'] = 1\n",
    "df_stock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753c906d-83cf-4b15-9485-37ec0ee15984",
   "metadata": {},
   "source": [
    "## Visualization (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84466d9-fa31-4f5d-93dd-7cf6297f2de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot candlestick graph, showing the change in stock price of AAPL over time\n",
    "fig = go.Figure(data=[go.Candlestick(x=df_stock['Date'], open=df_stock['Open'],\n",
    "                high=df_stock['High'],\n",
    "                low=df_stock['Low'],\n",
    "                close=df_stock['Close'])])\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=1500,\n",
    "    height=800,)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64cc6e9-8e42-497a-acc9-d3e88eceee50",
   "metadata": {},
   "source": [
    "# News articles dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb630ee-b011-4509-a2d0-03d610e609f8",
   "metadata": {},
   "source": [
    "## Load news articles data (read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b042bb2-9a4b-4135-b0d1-e702dbe88c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the newsarticles dataset\n",
    "df_text = pd.read_csv(\"../data/raw/us_equities_news_dataset.csv\")\n",
    "df_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fc12fa-569f-468b-bd31-b79102d2b519",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_text.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623a49c8-a166-4a5e-9778-8e131e101758",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54831c7e-d4bc-4dee-84d5-5da9cb5cdb45",
   "metadata": {},
   "source": [
    "### Define corpus (document filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684b395f-f342-47ec-b537-a5516d2cb7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect and count articles with titles relevant to Apple\n",
    "print(\"There are {} documents of which the title contains the word Apple\".format(str(df_text['title'].str.contains('[Aa]pple').sum())))\n",
    "print(\"There are {} documents of which the title contains the word iPad\".format(str(df_text['title'].str.contains('i[Pp]ad').sum())))\n",
    "print(\"There are {} documents of which the title contains the word iPod\".format(str(df_text['title'].str.contains('i[Pp]od').sum())))\n",
    "print(\"There are {} documents of which the title contains the word iPhone\".format(str(df_text['title'].str.contains('i[Pp]hone').sum())))\n",
    "print(\"There are {} documents of which the title contains the word Steve Jobs\".format(str(df_text['title'].str.contains('[Ss]teve [Jj]obs').sum())))\n",
    "print(\"There are {} documents of which the title contains the word Tim Cook\".format(str(df_text['title'].str.contains('[Tt]im [Cc]ook').sum())))\n",
    "print(\"There are {} documents of which the title contains the word iOS\".format(str(df_text['title'].str.contains('[Ii][Oo][Ss]').sum())))\n",
    "print(\"There are {} documents of which the title contains the word MacOS\".format(str(df_text['title'].str.contains('[Mm]ac[Oo][Ss]').sum())))\n",
    "print(\"There are {} documents of which the title contains the word Macbook\".format(str(df_text['title'].str.contains('[Mm]ac[Bb]ook').sum())))\n",
    "print(\"There are {} documents of which the title contains the word Mac Pro\".format(str(df_text['title'].str.contains('[Mm]ac [Pp]ro').sum())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c71dde-1f8c-4128-9379-b34e2ea822bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_idx = [] # index-list of indexnumbers that include above title RegEx\n",
    "for i in ['[Aa]pple', 'i[Pp]ad', 'i[Pp]od', 'i[Pp]hone', '[Ss]teve [Jj]obs', '[Tt]im [Cc]ook']:\n",
    "    idx_list = df_text[df_text['title'].str.contains(i)].index\n",
    "    for idx in idx_list:\n",
    "        if idx not in doc_idx:\n",
    "            doc_idx.append(idx)\n",
    "\n",
    "print(f\"The index-list now includes {len(np.unique(doc_idx))} unique articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bbbac8-4882-4ca4-a9a6-c3eeddc659f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Append all indices of articles that contain the AAPL stock ticker to the index-list\n",
    "for idx in list(df_text.loc[df_text['ticker'] == 'AAPL'].index):\n",
    "    doc_idx.append(idx)\n",
    "    \n",
    "print(f\"The index-list now includes {len(np.unique(doc_idx))} unique articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e455cb-9864-4046-9902-5c70ab5f89b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_idx = np.unique(doc_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbe7893-2dee-473a-b13b-41c19d57478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep articles relevant to Apple in the corpus and reset index of the resulting dataframe\n",
    "corpus = df_text.iloc[doc_idx].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743348a4",
   "metadata": {},
   "source": [
    "### Add stock movement label & match dates\n",
    "- Add stock movement label to each news article in corpus.\n",
    "    - Additionaly, it sorts on the date\n",
    "    - Additionaly, it matches the dates for the two data source because of the inner join\n",
    "    - This means we only include days that have both news articles and (recorded) stock movements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264e2f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpus.merge(df_stock.loc[:,['Date','Label']], on=None, left_on='release_date', right_on='Date', copy=False, sort=True).drop(['Date'], axis=1)\n",
    "display(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549794a8",
   "metadata": {},
   "source": [
    "### Remove duplicate documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb53792",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{corpus.shape[0]} original number of articles\")\n",
    "corpus.loc[corpus.astype(str).drop_duplicates().index]\n",
    "print(f\"{corpus.shape[0]} after dropping perfect duplicates\")\n",
    "corpus.drop_duplicates(subset=['content'], inplace=True)\n",
    "print(f\"{corpus.shape[0]} after dropping duplicates on content\")\n",
    "corpus.drop_duplicates(subset=['title','release_date'], inplace=True)\n",
    "print(f\"{corpus.shape[0]} after dropping duplicates on title and release_date only\")\n",
    "corpus.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8b6ebf",
   "metadata": {},
   "source": [
    "### Contractions removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d59c225",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(corpus)):\n",
    "    corpus.iloc[[i],[4]] = contractions.fix(corpus.iloc[i,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6980be5",
   "metadata": {},
   "source": [
    "### Remove markup & disclaimers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea8dccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def striphtml(data):\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', data)\n",
    "\n",
    "def strip_newline(data):\n",
    "    p = re.compile('\\n')\n",
    "    return p.sub(' ', data)\n",
    "\n",
    "def strip_disclaimer(data):\n",
    "    p = re.compile('[Aa]t the time of publication [\\w\\s]+did not own any direct investments in securities mentioned in this article')\n",
    "    pp = re.compile('[Hh]e may be an owner indirectly as an investor in a fund')\n",
    "    ppp = re.compile('[Ff]or previous columns [\\w\\s]+customers can click on')\n",
    "    pppp = re.compile('[Tt]he opinions expressed are his own')\n",
    "    temp = p.sub('', data)\n",
    "    temp = pp.sub('', temp)\n",
    "    temp = ppp.sub('', temp)\n",
    "    temp = pppp.sub('', temp)\n",
    "    return temp\n",
    "\n",
    "\n",
    "for i in range(len(corpus)):    \n",
    "    corpus.iloc[[i],[4]] = striphtml(corpus.iloc[i,4])\n",
    "    corpus.iloc[[i],[4]] = strip_newline(corpus.iloc[i,4])\n",
    "    corpus.iloc[[i],[4]] = strip_disclaimer(corpus.iloc[i,4])\n",
    "    corpus.iloc[[i],[4]] = re.sub('[^a-zA-Z]', ' ',corpus.iloc[i,4]) # remove any further special characters\n",
    "    corpus.iloc[[i],[4]] = re.sub('  +', ' ',corpus.iloc[i,4]) # remove extra spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fd5691-9934-4cb1-9ce1-b0c739c446ec",
   "metadata": {},
   "source": [
    "### Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa5df54-ef2f-42ea-987b-fa3c5dc344e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes about half a minute\n",
    "corpus['unigrams'] = corpus.apply(lambda row: nltk.word_tokenize(row['content']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32225511-9d11-4ed0-b78e-69c98da906e2",
   "metadata": {},
   "source": [
    "### Removing stop words & lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445946c0-4efb-4bbf-9d66-51c0b4b3f354",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "# add words from standard intros and endings\n",
    "more_stopwords = [\"columnist\", \"expressed\", \"eric\", \"editing\"]\n",
    "for word in more_stopwords:\n",
    "    stop_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f5c5fe-26a3-4d2c-815e-938f5457eabd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx, document in corpus.iterrows():\n",
    "    no_stop_words= [token.lower () for token in document['unigrams'] if not token.lower () in stop_words]\n",
    "    corpus.at[idx, 'unigrams'] = no_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0c4095-f4ad-4f09-a62b-94e0e44a1416",
   "metadata": {},
   "source": [
    "### Punctuation removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902facde-5c29-40d7-8191-15b67c5a21a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx, document in corpus.iterrows():\n",
    "    no_punctuation = [token for token in document['unigrams'] if token.isalpha()]\n",
    "    corpus.at[idx, 'unigrams'] = no_punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469e65e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Remove nonsensical words (NOT IN USE CURRENTLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4893194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# english_words = set(words.words())\n",
    "# custom_words = [\"apple\", \"ipad\", \"ipod\", \"iphone\", \"macos\", \"osx\", \"macbook\", \"steve\", \"jobs\", \"tim\", \"cook\"]\n",
    "# for word in custom_words:\n",
    "#     english_words.add(word)\n",
    "\n",
    "# for idx, document in corpus.iterrows():\n",
    "#     no_nonsense = [token for token in document['unigrams'] if token in english_words]\n",
    "#     corpus.at[idx, 'unigrams'] = no_nonsense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dc72a0",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5a728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization (takes about 16 seconds)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "custom_words = [\"apple\", \"ipad\", \"ipod\", \"iphone\", \"macos\", \"osx\", \"macbook\", \"steve\", \"jobs\", \"tim\", \"cook\"]\n",
    "\n",
    "for idx, document in corpus.iterrows():\n",
    "    lemmings= [token if token in custom_words else lemmatizer.lemmatize(token) for token in document['unigrams']]\n",
    "    corpus.at[idx, 'unigrams'] = lemmings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044c16e2-7a43-4ff9-a84e-e81c92b8b940",
   "metadata": {},
   "source": [
    "## Preprocessed corpus exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6df3bc4-1dcf-45af-b686-d42017195234",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Corpus has shape: {corpus.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9e60b0-55df-4691-aede-769b4dd360ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.category.value_counts().plot(kind='bar',\n",
    "                                    color = ['royalblue', 'orange'],\n",
    "                                   title= f\"Corpus category distribution ({len(corpus)} documents total)\",\n",
    "                                   figsize = (15,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9f0b24-2592-4c27-8f21-6c199cbb6d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus.provider.value_counts().head(10))\n",
    "corpus.provider.value_counts()[:5].plot(kind='bar',\n",
    "                                        color='royalblue',\n",
    "                                   title= f\"Corpus top 5 provider distribution ({len(corpus)} documents total)\",\n",
    "                                       figsize=(15,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbf6bee",
   "metadata": {},
   "source": [
    "# Bag-of-Words (BoW) representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b544866-8ee1-466f-8f43-27e950c75b13",
   "metadata": {},
   "source": [
    "## BoW creation (Document-Term Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903ae9e7-398d-4d05-a1ac-8e4362e00839",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(tokenizer = lambda doc: doc, ngram_range=(1,1), lowercase=False)\n",
    "X = count_vectorizer.fit_transform(corpus['unigrams'][:])\n",
    "df_bow = pd.DataFrame(X.toarray(),columns=count_vectorizer.get_feature_names_out())\n",
    "df_bow.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a08f74f-e164-4cf8-95d5-0ed9b87ecad7",
   "metadata": {},
   "source": [
    "## BoW exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3bd6d0-2cea-40f4-8a70-ec16e1be10d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe based on word frequency\n",
    "df_bow_frequency = df_bow.sum(axis=0).sort_values(ascending = False)\n",
    "word_count_df = pd.DataFrame(zip(df_bow_frequency.index, df_bow_frequency.values), columns = ['Tag', 'Count'])\n",
    "display(word_count_df.head(10))\n",
    "print(word_count_df['Count'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c5afcb-6bd0-4689-aaf8-92ade43adc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top 10 most frequently occuring words\n",
    "word_count_df[:10].plot.bar(x='Tag',\n",
    "                           y='Count',\n",
    "                           color = ['royalblue'],\n",
    "                           title= f\"BoW top 10 most frequent words\",\n",
    "                           figsize = (15,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb71bd07-5989-41a5-918b-05ddbe20425d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_frequency_count_fraction(df, frequency_threshold, print_bool = False):\n",
    "    frequency_fraction_count = (df.Count <= frequency_threshold).sum() / len(df)\n",
    "    if print_bool == True:\n",
    "        print(f\"{frequency_fraction_count} % of the unique words only occur {frequency_threshold} time(s)\")\n",
    "    return frequency_fraction_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9312a49-6474-4d91-af3d-6c6725dfa460",
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_list = []\n",
    "for i in range (0, 100):\n",
    "    fraction_count = calculate_frequency_count_fraction(word_count_df, i)\n",
    "    frac_list.append(fraction_count)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.grid()\n",
    "plt.xticks(ticks=np.arange(0,100,5))\n",
    "plt.ylabel(\"Fractional proportion\")\n",
    "plt.xlabel(\"Maximum word frequency\")\n",
    "plt.title(\"Cumulative distribution of maximum word count-frequency over entire corpus\", size = 20)\n",
    "plt.plot(frac_list, marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46510685-1a1c-4030-9628-351b0ef8927e",
   "metadata": {},
   "source": [
    "## BoW filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb0d8fe-f832-4067-a5e4-7fe9304b2ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(tokenizer = lambda doc: doc, ngram_range=(1,1), lowercase=False, min_df = 15, max_df = 0.95)\n",
    "X = count_vectorizer.fit_transform(corpus['unigrams'][:])\n",
    "X = X.toarray()\n",
    "df_bow = pd.DataFrame(X,columns=count_vectorizer.get_feature_names_out())\n",
    "df_bow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb88655c-4fc2-4087-87fb-c6340bc73280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe based on word frequency\n",
    "df_bow_frequency = df_bow.sum(axis=0).sort_values(ascending = False)\n",
    "word_count_df = pd.DataFrame(zip(df_bow_frequency.index, df_bow_frequency.values), columns = ['Tag', 'Count'])\n",
    "display(word_count_df.head(10))\n",
    "print(word_count_df['Count'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4bf5fc-2542-4394-a684-d834d4dbe4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tf-idf representation using the bag-of-words matrix\n",
    "tfidf_transform = sklearn.feature_extraction.text.TfidfTransformer(norm=None)\n",
    "X_tfidf = tfidf_transform.fit_transform(df_bow)\n",
    "\n",
    "# This is a document x words matrix\n",
    "X_tfidf\n",
    "\n",
    "X_tfidf_array = X_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993d1bee",
   "metadata": {},
   "source": [
    "# Train & Evaluate\n",
    "- What is the experimental design used for training and evaluating?\n",
    "- How does the model perform? What metrics do we use and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d055898",
   "metadata": {},
   "source": [
    "## Cross-Validation for time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76768ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "X = X_tfidf_array\n",
    "y = corpus[\"Label\"]\n",
    "\n",
    "ts_cv = TimeSeriesSplit(\n",
    "    n_splits=5,                         # 5 might not make a lot of sense. Maybe better to do shorter time intervals -> n_splits = ~200\n",
    "    gap=0,\n",
    "    max_train_size=None,\n",
    "    test_size=None\n",
    ")\n",
    "\n",
    "all_splits = list(ts_cv.split(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d81af2",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d556b0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# To surpress the non-convergence warnings\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def evaluate(model, X, y, cv):\n",
    "    cv_results = cross_validate(\n",
    "        model,\n",
    "        X,\n",
    "        y,\n",
    "        cv=cv,\n",
    "        scoring=[\"accuracy\"],\n",
    "    )\n",
    "    acc = cv_results[\"test_accuracy\"]\n",
    "    fit_time = cv_results[\"fit_time\"]\n",
    "    print(\n",
    "        f\"===== {model} =====   [fit time: {sum(fit_time):.1f}s]\\n\"\n",
    "        f\"Accuracy:                {acc.mean():.3f} +/- {acc.std():.3f}\\n\"\n",
    "    )\n",
    "    return(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324ae75b-385d-4721-aa7d-3081940c0649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def gridsearch_count_vectorizer(df_bow):\n",
    "    \n",
    "    cv_results_all = []\n",
    "    ngrams = [(1,1),(1,2),(1,3), (2,2), (3,3)]\n",
    "    termfreqlist = [2,3]\n",
    "\n",
    "    test_df = pd.DataFrame(columns=['ngram','term_frequency','accuracy'])\n",
    "\n",
    "    for ngram in ngrams:\n",
    "        for termfreq in termfreqlist:\n",
    "\n",
    "            count_vectorizer = CountVectorizer(tokenizer = lambda doc: doc, ngram_range=(1,1), lowercase=False, min_df = 15, max_df = 0.95)\n",
    "            X = count_vectorizer.fit_transform(corpus['unigrams'][:])\n",
    "            X = X.toarray()\n",
    "\n",
    "\n",
    "            #create a model instance and split dataset to train and test sets\n",
    "            model = LogisticRegression()\n",
    "            cv_results = evaluate(model, X, y, ts_cv)\n",
    "            cv_results_all.append(cv_results)\n",
    "            \n",
    "            \n",
    "            res_dict = {'ngram':ngram,'feature_num':feat,'accuracy':classification['accuracy']}\n",
    "            res_df = pd.DataFrame([res_dict])\n",
    "            test_df = pd.concat([test_df,res_df], ignore_index=True, axis=0)\n",
    "            \n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd07f1f-33e2-4470-b54b-3f405d61ff65",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = gridsearch_count_vectorizer(df_bow)\n",
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313ce875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier, Perceptron\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "models = [LogisticRegression(), SGDClassifier(), Perceptron(), LogisticRegression(solver='saga',penalty='l1'), RandomForestClassifier()] #RidgeClassifier takes ~2min to run\n",
    "cv_results_all = []\n",
    "\n",
    "for model in models:\n",
    "    cv_results = evaluate(model, X, y, ts_cv)\n",
    "    cv_results_all.append(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64645e11",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "vscode": {
   "interpreter": {
    "hash": "794c3e2d85566e5cd2c84b64a14258bc3bc8401757de0e297305ca2d270fd73c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
