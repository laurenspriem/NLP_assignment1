{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34da11a5-7346-413c-9f64-be7b1e3ef676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import FuzzyTM\n",
    "import contractions\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef432a5",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "- What documents do we include in the corpus?\n",
    "- How do we create the labels for the target variable?\n",
    "- Has the corpus from Kaggle been pre-processed in some way? Do we use other proprocessing steps as well? (use at least stop-word removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0b16db-49d4-42e9-9a4e-34939773d08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock = pd.read_csv(\"../data/raw/AAPL.csv\")\n",
    "\n",
    "df_stock['Label'] = 0\n",
    "df_stock.loc[(df_stock['Close'] - df_stock['Open']) > 0, 'Label'] = 1\n",
    "df_stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84466d9-fa31-4f5d-93dd-7cf6297f2de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[go.Candlestick(x=df_stock['Date'], open=df_stock['Open'],\n",
    "                high=df_stock['High'],\n",
    "                low=df_stock['Low'],\n",
    "                close=df_stock['Close'])])\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=1500,\n",
    "    height=800,)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b042bb2-9a4b-4135-b0d1-e702dbe88c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = pd.read_csv(\"../data/raw/us_equities_news_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040e05e1-fc26-4111-b51e-062554858c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54831c7e-d4bc-4dee-84d5-5da9cb5cdb45",
   "metadata": {},
   "source": [
    "## Select relevant documents (corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684b395f-f342-47ec-b537-a5516d2cb7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are {} documents of which the title contains the word Apple\".format(str(df_text['title'].str.contains('[Aa]pple').sum())))\n",
    "print(\"There are {} documents of which the title contains the word iPad\".format(str(df_text['title'].str.contains('i[Pp]ad').sum())))\n",
    "print(\"There are {} documents of which the title contains the word iPod\".format(str(df_text['title'].str.contains('i[Pp]od').sum())))\n",
    "print(\"There are {} documents of which the title contains the word iPhone\".format(str(df_text['title'].str.contains('i[Pp]hone').sum())))\n",
    "print(\"There are {} documents of which the title contains the word Steve Jobs\".format(str(df_text['title'].str.contains('[Ss]teve [Jj]obs').sum())))\n",
    "print(\"There are {} documents of which the title contains the word Tim Cook\".format(str(df_text['title'].str.contains('[Tt]im [Cc]ook').sum())))\n",
    "print(\"There are {} documents of which the title contains the word iOS\".format(str(df_text['title'].str.contains('[Ii][Oo][Ss]').sum())))\n",
    "print(\"There are {} documents of which the title contains the word MacOS\".format(str(df_text['title'].str.contains('[Mm]ac[Oo][Ss]').sum())))\n",
    "print(\"There are {} documents of which the title contains the word Macbook\".format(str(df_text['title'].str.contains('[Mm]ac[Bb]ook').sum())))\n",
    "print(\"There are {} documents of which the title contains the word Mac Pro\".format(str(df_text['title'].str.contains('[Mm]ac [Pp]ro').sum())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c71dde-1f8c-4128-9379-b34e2ea822bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_idx = [] # list of indexnumbers that include above titel RegEx\n",
    "for i in ['[Aa]pple', 'i[Pp]ad', 'i[Pp]od', 'i[Pp]hone', '[Ss]teve [Jj]obs', '[Tt]im [Cc]ook']:\n",
    "    idx_list = df_text[df_text['title'].str.contains(i)].index\n",
    "    for idx in idx_list:\n",
    "        if idx not in doc_idx:\n",
    "            doc_idx.append(idx)\n",
    "            \n",
    "print(len(np.unique(doc_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bbbac8-4882-4ca4-a9a6-c3eeddc659f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx in list(df_text.loc[df_text['ticker'] == 'AAPL'].index):\n",
    "    doc_idx.append(idx)\n",
    "    \n",
    "print(len(np.unique(doc_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e455cb-9864-4046-9902-5c70ab5f89b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_idx = np.unique(doc_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbe7893-2dee-473a-b13b-41c19d57478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df_text.iloc[doc_idx].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743348a4",
   "metadata": {},
   "source": [
    "## Add stock movement label & match dates\n",
    "- Add stock movement label to each news article in corpus.\n",
    "    - Additionaly, it sorts on the date\n",
    "    - Additionaly, it matches the dates for the two data source because of the inner join\n",
    "    - This means we only include days that have both news articles and (recorded) stock movements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264e2f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpus.merge(df_stock.loc[:,['Date','Label']], on=None, left_on='release_date', right_on='Date', copy=False, sort=True).drop(['Date'], axis=1)\n",
    "corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549794a8",
   "metadata": {},
   "source": [
    "## Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb53792",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{corpus.shape[0]} original number of articles\")\n",
    "corpus.loc[corpus.astype(str).drop_duplicates().index]\n",
    "print(f\"{corpus.shape[0]} after dropping perfect duplicates\")\n",
    "corpus.drop_duplicates(subset=['content'], inplace=True)\n",
    "print(f\"{corpus.shape[0]} after dropping duplicates on content\")\n",
    "corpus.drop_duplicates(subset=['title','release_date'], inplace=True)\n",
    "print(f\"{corpus.shape[0]} after dropping duplicates on title and release_date only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8b6ebf",
   "metadata": {},
   "source": [
    "## Contractions removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d59c225",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(corpus)):\n",
    "    corpus.iloc[[i],[4]] = contractions.fix(corpus.iloc[i,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fd5691-9934-4cb1-9ce1-b0c739c446ec",
   "metadata": {},
   "source": [
    "## Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa5df54-ef2f-42ea-987b-fa3c5dc344e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus['unigrams'] = corpus.apply(lambda row: nltk.word_tokenize(row['content']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2853e318",
   "metadata": {},
   "source": [
    "## Remove article markup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4618016f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus.iloc[0,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c705cd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus.iloc[0,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb61cb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove \\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32225511-9d11-4ed0-b78e-69c98da906e2",
   "metadata": {},
   "source": [
    "## Removing stop words & lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445946c0-4efb-4bbf-9d66-51c0b4b3f354",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f5c5fe-26a3-4d2c-815e-938f5457eabd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx, document in corpus.iterrows():\n",
    "    #print(len(document['unigrams']))\n",
    "    no_stop_words= [token.lower () for token in document['unigrams'] if not token.lower () in stop_words]\n",
    "    #print(len(document['unigrams']))\n",
    "    corpus.at[idx, 'unigrams'] = no_stop_words\n",
    "    #print(\"====\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0c4095-f4ad-4f09-a62b-94e0e44a1416",
   "metadata": {},
   "source": [
    "## Punctuation removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902facde-5c29-40d7-8191-15b67c5a21a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx, document in corpus.iterrows():\n",
    "    #print(len(document['unigrams']))\n",
    "    no_punctuation = [token for token in document['unigrams'] if token.isalpha()]\n",
    "    corpus.at[idx, 'unigrams'] = no_punctuation\n",
    "    #print(len(document['unigrams']))\n",
    "    #print(\"======\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469e65e6",
   "metadata": {},
   "source": [
    "## Remove nonsensical words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4893194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_words = set(words.words())\n",
    "custom_words = [\"apple\", \"ipad\", \"ipod\", \"iphone\", \"macos\", \"osx\", \"macbook\", \"steve\", \"jobs\", \"tim\", \"cook\"]\n",
    "for word in custom_words:\n",
    "    english_words.add(word)\n",
    "\n",
    "for idx, document in corpus.iterrows():\n",
    "    #print(len(document['unigrams']))\n",
    "    no_nonsense = [token for token in document['unigrams'] if token in english_words]\n",
    "    corpus.at[idx, 'unigrams'] = no_nonsense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dc72a0",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5a728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization (takes about 16 seconds)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for idx, document in corpus.iterrows():\n",
    "    #print(len(document['unigrams']))\n",
    "    lemmings= [token if token in custom_words else lemmatizer.lemmatize(token) for token in document['unigrams']]\n",
    "    #print(len(document['unigrams']))\n",
    "    corpus.at[idx, 'unigrams'] = lemmings\n",
    "    #print(\"====\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c148c5-694b-4ff3-9d15-e20fff27f6de",
   "metadata": {},
   "source": [
    "## Top word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4805b309-a998-4ac0-9754-f3d17efb540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counted_doc = Counter(corpus['unigrams'][2])\n",
    "counted_doc.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbf6bee",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "- Use BoW representation. But there are still many modeling decisions to be made. Motivate these decisions, pay attention to feature selection and the measures/metrics you use to represent the words/text.\n",
    "- Which classification algorithm do we choose? Why?\n",
    "- Which parameters do we use? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ff5061-54c2-49ce-a947-ff13fa9c18bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "\n",
    "X = vectorizer.fit_transform(corpus['unigrams'][:])\n",
    "df_bow = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "df_bow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6d85db-12b6-401a-aec2-987c52b88cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_bow.columns[1000:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ceedad-fd11-49f4-b5bf-6bc79e5b9781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe low frequency words\n",
    "df_bow.sum(axis=0).sort_values(ascending = True)[:int(len(df_bow.columns)*0.3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2f5bce-71de-4451-8acd-d120dbe13fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe high frequency words\n",
    "df_bow.sum(axis=0).sort_values(ascending = False)[:int(len(df_bow.columns)*0.3)][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db961c49-e67f-402d-a6a4-931848dfaef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tf-idf representation using the bag-of-words matrix\n",
    "tfidf_transform = sklearn.feature_extraction.text.TfidfTransformer(norm=None)\n",
    "X_tfidf = tfidf_transform.fit_transform(df_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a09c84-a77b-46b2-92c5-c8fd9a15260e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a document x words matrix\n",
    "X_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f317cff-37eb-4481-8821-3093efd5007c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_array = X_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993d1bee",
   "metadata": {},
   "source": [
    "# Train & Evaluate\n",
    "- What is the experimental design used for training and evaluating?\n",
    "- How does the model perform? What metrics do we use and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d055898",
   "metadata": {},
   "source": [
    "## Cross-Validation for time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76768ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "X = X_tfidf\n",
    "y = corpus[\"Label\"]\n",
    "\n",
    "ts_cv = TimeSeriesSplit(\n",
    "    n_splits=5,                         # 5 might not make a lot of sense. Maybe better to do shorter time intervals -> n_splits = ~200\n",
    "    gap=0,\n",
    "    max_train_size=None,\n",
    "    test_size=None,\n",
    ")\n",
    "\n",
    "all_splits = list(ts_cv.split(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d81af2",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d556b0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# To surpress the non-convergence warnings\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def evaluate(model, X, y, cv):\n",
    "    cv_results = cross_validate(\n",
    "        model,\n",
    "        X,\n",
    "        y,\n",
    "        cv=cv,\n",
    "        scoring=[\"accuracy\",\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"],\n",
    "    )\n",
    "    acc = cv_results[\"test_accuracy\"]\n",
    "    mae = -cv_results[\"test_neg_mean_absolute_error\"]\n",
    "    rmse = -cv_results[\"test_neg_root_mean_squared_error\"]\n",
    "    fit_time = cv_results[\"fit_time\"]\n",
    "    print(\n",
    "        f\"===== {model} =====   [fit time: {sum(fit_time):.1f}s]\\n\"\n",
    "        f\"Accuracy:                {acc.mean():.3f} +/- {acc.std():.3f}\\n\"\n",
    "        f\"Mean Absolute Error:     {mae.mean():.3f} +/- {mae.std():.3f}\\n\"\n",
    "        f\"Root Mean Squared Error: {rmse.mean():.3f} +/- {rmse.std():.3f}\"\n",
    "    )\n",
    "    return(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313ce875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier, Perceptron\n",
    "\n",
    "models = [LogisticRegression(), SGDClassifier(), Perceptron()] #RidgeClassifier takes ~2min to run\n",
    "cv_results_all = []\n",
    "\n",
    "for model in models:\n",
    "    cv_results = evaluate(model, X, y, ts_cv)\n",
    "    cv_results_all.append(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64645e11",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "vscode": {
   "interpreter": {
    "hash": "794c3e2d85566e5cd2c84b64a14258bc3bc8401757de0e297305ca2d270fd73c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
